{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression on YearPredictionMSD\n",
    "\n",
    "In this notebook we are going to train a linear regression model on the YearPredictionMSD, a data set that contains features extracted from songs. The model that we will train is going to predict which year a song was from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate ZipML_SGD\n",
    "\n",
    "First, we instantiate a ZipML_SGD object, that we will use for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floatFSGD.bit is loaded\n",
      "Got sgd_ctrl!\n",
      "Got sgd_dma!\n",
      "Allocated buffer of size: 33554432 bytes\n"
     ]
    }
   ],
   "source": [
    "import zipml\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "Z = zipml.ZipML_SGD(on_pynq=1, bitstreams_path=zipml.BITSTREAMS, ctrl_base=0x41200000, dma_base=0x40400000, dma_buffer_size=32*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the data\n",
    "\n",
    "We load the data that is formatted in libsvm format (label feature_index1:feature1 feature_index2:feature2 ...). Then, we perform a normalization on the features of the data set, where we can specify: (1) The normalization range (-1 to 1 or 0 to 1), (2) if the normalization should happen row-wise or column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-10-30 09:21:17--  https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/YearPredictionMSD.bz2\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 212177525 (202M) [application/x-bzip2]\n",
      "Saving to: ‘YearPredictionMSD.bz2’\n",
      "\n",
      "YearPredictionMSD.b 100%[=====================>] 202.35M  3.19MB/s   in 40s    \n",
      "\n",
      "2017-10-30 09:21:58 (5.11 MB/s) - ‘YearPredictionMSD.bz2’ saved [212177525/212177525]\n",
      "\n",
      "rm: cannot remove ‘YearPredictionMSD.bz2’: No such file or directory\n",
      "--2017-10-30 09:26:33--  https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/YearPredictionMSD.t.bz2\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23639653 (23M) [application/x-bzip2]\n",
      "Saving to: ‘YearPredictionMSD.t.bz2’\n",
      "\n",
      "YearPredictionMSD.t 100%[=====================>]  22.54M  5.70MB/s   in 5.7s   \n",
      "\n",
      "2017-10-30 09:26:40 (3.95 MB/s) - ‘YearPredictionMSD.t.bz2’ saved [23639653/23639653]\n",
      "\n",
      "rm: cannot remove ‘YearPredictionMSD.t.bz2’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!./get_yearpredictionMSD.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_values_one_row: 94\n",
      "a loaded, time: 1150.6679298877716\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Z.load_libsvm_data('./YearPredictionMSD/YearPredictionMSD', 50000, 90)\n",
    "print('a loaded, time: ' + str(time.time()-start) )\n",
    "Z.a_normalize(to_min1_1=1, row_or_column='c');\n",
    "Z.b_normalize(to_min1_1=0)\n",
    "\n",
    "# Set training related parameters\n",
    "num_epochs = 10\n",
    "step_size = 1.0/(1 << 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full precision training using the CPU\n",
    "\n",
    "Let's perform training on full-precision data using the CPU. During the training, we calculate the loss after each epoch (a complete scan over the data set). The convergence of the model is observed by the decreasing loss. After having performed the given number of epochs, we save the trained model in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed linear regression on cadata. Training time: 110.11404371261597\n",
      "Initial loss: 0.384345112242\n",
      "Epoch 0 loss: 0.00861334713324\n",
      "Epoch 1 loss: 0.00805644661588\n",
      "Epoch 2 loss: 0.00771162294714\n",
      "Epoch 3 loss: 0.00746739865464\n",
      "Epoch 4 loss: 0.00728293887823\n",
      "Epoch 5 loss: 0.0071388712119\n",
      "Epoch 6 loss: 0.00702395660503\n",
      "Epoch 7 loss: 0.0069308542541\n",
      "Epoch 8 loss: 0.00685445351566\n",
      "Epoch 9 loss: 0.00679106218299\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Train on the CPU\n",
    "x_history = Z.LINREG_SGD(num_epochs=num_epochs, step_size=step_size, regularize=0)\n",
    "\n",
    "print('Performed linear regression on cadata. Training time: ' + str(time.time()-start))\n",
    "# Print losses after each epoch\n",
    "initial_loss = Z.calculate_LINREG_loss(np.zeros(Z.num_features), 0)\n",
    "print('Initial loss: ' + str(initial_loss))\n",
    "for e in range(0, num_epochs):\n",
    "\tloss = Z.calculate_LINREG_loss(x_history[:,e], 0)\n",
    "\tprint('Epoch ' + str(e) + ' loss: ' + str(loss) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full-precision training using the FPGA\n",
    "\n",
    "Now, let's use the ZipML-PYNQ overlay to perform the same training process on the FPGA. We just replace the CPU-training function with the FPGA one. The training is 2 order of magnitudes faster on the FPGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x0\n",
      "0xbf800000\n",
      "0xbf800000\n",
      "0x0\n",
      "0x5b\n",
      "0xc350\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 4700000\n",
      "bytes_for_model: 368\n",
      "FPGA train time: 0.330078125\n",
      "Initial loss: 0.384345112242\n",
      "Epoch 0 loss: 0.00862058925779\n",
      "Epoch 1 loss: 0.00806469953043\n",
      "Epoch 2 loss: 0.00772012776682\n",
      "Epoch 3 loss: 0.00747571889878\n",
      "Epoch 4 loss: 0.00729096349781\n",
      "Epoch 5 loss: 0.00714612406087\n",
      "Epoch 6 loss: 0.00703085683972\n",
      "Epoch 7 loss: 0.00693696086218\n",
      "Epoch 8 loss: 0.00685999522279\n",
      "Epoch 9 loss: 0.00679613550255\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Train on FPGA\n",
    "Z.configure_SGD_FPGA(num_epochs, step_size, -1, -1, 0, 0)\n",
    "x_history = Z.SGD_FPGA(num_epochs)\n",
    "\n",
    "print('FPGA train time: ' + str(time.time()-start) )\n",
    "# Print losses after each epoch\n",
    "initial_loss = Z.calculate_LINREG_loss(np.zeros(Z.num_features), 0)\n",
    "print('Initial loss: ' + str(initial_loss))\n",
    "for e in range(0, num_epochs):\n",
    "\tloss = Z.calculate_LINREG_loss(x_history[:,e], 0)\n",
    "\tprint('Epoch ' + str(e) + ' loss: ' + str(loss) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Low-precision training using the FPGA\n",
    "\n",
    "With the ZipML-PYNQ overlay, we can even improve upon previous results by using low-precision data. We call the quantization function that compresses the features of the data set using deterministic quantization. The rest of the code remains exactly the same as the previous example. We can see that the total training time is improved upon the full precision variant, while the convergence quality is kept. You can experiment with different precisions and most of the time you might see that 8-bits are enough to get to the same quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_levels: 129\n",
      "qFSGD8.bit is loaded\n",
      "num_quantized_items_in_word: 4.0\n",
      "mask: 255\n",
      "self.total_size: 1300000\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x0\n",
      "0xbf800000\n",
      "0xbf800000\n",
      "0x0\n",
      "0x5b\n",
      "0xc350\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 4700000, self.data_end_index: 6000000\n",
      "bytes_for_model: 448\n",
      "FPGA train time: 0.11707735061645508\n",
      "Initial loss: 0.384345112242\n",
      "Epoch 0 loss: 0.00857622252748\n",
      "Epoch 1 loss: 0.00800754022974\n",
      "Epoch 2 loss: 0.00766088894158\n",
      "Epoch 3 loss: 0.00741871949842\n",
      "Epoch 4 loss: 0.00723901676682\n",
      "Epoch 5 loss: 0.00710134055793\n",
      "Epoch 6 loss: 0.0069939509613\n",
      "Epoch 7 loss: 0.00690964300533\n",
      "Epoch 8 loss: 0.00684188243595\n",
      "Epoch 9 loss: 0.00678810862973\n"
     ]
    }
   ],
   "source": [
    "Z.a_quantize(quantization_bits=8)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Train on FPGA\n",
    "Z.configure_SGD_FPGA(num_epochs, step_size, -1, -1, 0, 0)\n",
    "x_history = Z.SGD_FPGA(num_epochs)\n",
    "\n",
    "print('FPGA train time: ' + str(time.time()-start) )\n",
    "# Print losses after each epoch\n",
    "initial_loss = Z.calculate_LINREG_loss(np.zeros(Z.num_features), 0)\n",
    "print('Initial loss: ' + str(initial_loss))\n",
    "for e in range(0, num_epochs):\n",
    "\tloss = Z.calculate_LINREG_loss(x_history[:,e], 0)\n",
    "\tprint('Epoch ' + str(e) + ' loss: ' + str(loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
