{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Training on MNIST\n",
    "\n",
    "In this notebook, we are going to train an L2-SVM model on PYNQ using stochasting gradient descent optimization, to classify handwritten digits of the MNIST data set. We are going to perform training on both the CPU and the FPGA, comparing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate ZipML_SGD\n",
    "\n",
    "First, we instantiate a ZipML_SGD object, that we will use for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floatFSGD.bit is loaded\n",
      "Got sgd_ctrl!\n",
      "Got sgd_dma!\n",
      "Allocated buffer of size: 33554432 bytes\n"
     ]
    }
   ],
   "source": [
    "import zipml\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "Z = zipml.ZipML_SGD(on_pynq=1, bitstreams_path=zipml.BITSTREAMS, ctrl_base=0x41200000, dma_base=0x40400000, dma_buffer_size=32*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the data\n",
    "\n",
    "We load the data that is formatted in libsvm format (label feature_index1:feature1 feature_index2:feature2 ...).\n",
    "Then, we perform a normalization on the features of the data set, where we can specify: (1) The normalization range (-1 to 1 or 0 to 1), (2) if the normalization should happen row-wise or column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mnist] Nothing to do.\r\n",
      "[mnist.t] Nothing to do.\r\n"
     ]
    }
   ],
   "source": [
    "!./get_mnist.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_values_one_row: 788\n",
      "Data loaded, time: 33.774189949035645\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Z.load_libsvm_data('./mnist/mnist', 10000, 784)\n",
    "print('Data loaded, time: ' + str(time.time()-start) )\n",
    "Z.a_normalize(to_min1_1=0, row_or_column='r') # Normalize features of the data set\n",
    "\n",
    "# Set training related parameters\n",
    "num_epochs = 10\n",
    "step_size = 1.0/(1 << 12)\n",
    "cost_pos = 1.0\n",
    "cost_neg = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our date set, to take a look at the handwritten digits and to make sure if we have loaded it correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB5ElEQVR4nF2SMUxTURSGv3Pvue+9\nvvdKWyBIDW4mhkEm46ZMJjo54OBgnB2IiZuzrqzKJHFzcSFBExeNS2PiogwkbjKIIQrEkNJC+3oc\n2lfBf7k39+T/z3//cwARHzyACAhoQoMRnCtvIiIeBKiE8qEAnNpgYEDcU40PxADw4Byi4h04LyOV\nfCzqxfpmgBUWmKSuDJkRyLBriPKZ2alHa6+bq2aPAbQAGSAT1bmLlxYW5pTet9XbR60WgID4Pnmz\nVY1N4ETk4a/O7skmY7iQbFlv39qbG8edIMToqaJGS88emH1S5lc8cUrGSBfEaWWKF7/vBziPI4w4\nIoANis5euj91t0G+IzG9vMxseCakvLOrwLR4Sm6JQHblaPvVcg45pw0BPqRwzbq23BRwZ4l4R1K5\n/MaOnp/DEw3jc3hQUAVcds9sXU+1jICkAlnsE8UGO7eE6rCSKtRSNAHH/NMPB/3vhKwkavCQQB4u\nrOwdFh17C0yOVcMEkmt9edvaZl9vVMb/yEIMQsPfaRWFtd/frAFRY+wmz2mu/TzsmH1cqsOMRlEZ\nQlCur29Zt2+7T2aBmARqQzeux/TiYq3z5fOPl+02vjimC39GwWQxVCKqTKOUy/cP6iACh/w3i/Hm\nV3Pn0bOj+gvaWYWyqzCuGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x2F444790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABt0lEQVR4nL2SzWsTURTFf/fNm/cm\nM0kTYv0WKSLouiBEXFgQ0V2lWYj4F/iB/gUFF4q4EhfiSnFVN4WCCy2iIuhK6VooooJGJa1f1MYk\no3NdZBJN/gDP9rxzzzv3XOgjFixE2EhIhCEUoQrjJGAKUBhmQ7DgKIZ4K8EQJziXvymBZxSBx9fm\n9GaNzYyYbgTPgVdNTVvhqDLCxxx+odp+2aoVYjs61h55rt+7i6ezbPafX5bAeqL5FVXVlXMP2wvh\nUATj7ORXXXt0Sht7j+mt3FOMgAiFHZ/0xzNfP7NLjDb3DazAwu55fb1c7yXUdG5gKgLFu9m76VJS\n8mwF7Tx2ABgsAkez9sEy4wFA3NJ7Lh9qMOxc+PUUgO0VqLQ+L+U7Q7BMaXY2xsSAraJ6OScDEHei\n/WFbQBjGCMmFdHFDvw3rA6azN2UIQDwXtTmF6SmNAY7rVQxxQrDnSed6GA2SWHAnfy5vARg73/x9\nh79bFx+I1NfT2UObZh68X2/c3l+mmncWgDPMqK423nZ0bemSg0pf6wQMk/e/aPdb+vFKgE3GcP22\no5DYMnFtVbs3JqJeTX74vv4n/gDl63znoBdMygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x2C15ACD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB3ElEQVR4nG2SzWoUQRSFv7r1190z\nGYhGh4wSUBfBnSDiRlxINgriC7jyIXwEFy7EB3ClIIIuRHwAF0YQUQQFwQgKgQgRN5m/nu7qKhc9\nM4rkrAq+W9Q5py451mMdC2WAAwOAIAB6SXMcIi2LnYiDuIRCJMryiNY6LZgbEwp0AYASALu8iFqH\n/r8Wuu3DAEaQGzsXOnM/gctbf2FYy4+s/twdt1AHrl43YQH1/vT3lh+07sVgvzyhAOMMCGT98eM2\npknBqB5MuuMAoFgZPis+BBMAcYQN65wdJXRuaRiqtfCiGwBklnHz2KiqM08zrYXjnNws/Ui1hizn\n+SyrZYWCzO9zz//6Ng9uPY/qi0BPPGTq1sNhuubnmWuyWXNmd8+e6Jw7e/qSr7cVe8HWAEbHMPb3\nfzi1qdzw9dO3z+OnzkepVQJMPgp3vl7pjHrv373ZmQC3+99tg162grFZXngQo3iQ7qJbP4Yekxio\nAUgBBrykmbfHwUEghyIzkJQwK6fFfFZsx0KJTMqAKBJTTgWl5lFCQhqwDSolwGUblY8RQJQCrYnE\n2ERUhuYo1G19CZgFihqcI5WUsbtSkS0XDJgAVQXwKvlhouEwWQaO9fxQRruT5O3P/y+vJFRIBP4A\nQ3OjED2bqnQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x3699DFD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABKUlEQVR4nHWSLU/EQBRFTzsz7exX\nWxIEJGtArMAgsCQEgyIhSDz8ADwCwU/AoEEi8ASFATRZsUEiyS4lLLQJ5SE2S5bpcOU9uXfmzRtw\npdBAo+YDBIBKlJcRaCJM4IcGYkBPWxzpKjIy9iZtfPRerNPy9+6PZbxtAQhd1kyb3zYv/pqKJCKG\nrRfpJ3NNJ5JBh87mQIYHMRjnJoDiQuRaMRlntla1sb1hMdgF3Fqjjc7uRY4J0toIGXAo+WXX7QR0\nQLrzPLrtkVJbioXlUqpzQhqKtpOMOMtltFo7bqKNp7K88qM4eCzkZsEPNSIfe+7LTHX6+SZd/P9j\nrZTyRM37gyuV3GWGbNb73edSzuLrl3r1woc+eWIr6+9tYfgHERK2Ddrd1USKjna9HxzLSSfvRWGs\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x2C15ACD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABvklEQVR4nF2SO2tUURSFv73PPufM\nMzGG+EALhZhoaxBs7EWwCoiNhfgbLERrQSwshfwA7SyTIIJgJVgpBBEhEZuQQsIgE/XeubMtZu4w\nk9UuztrrcWAMEwACGQQSs9CUFARyApuhogKICSFgzZFaTZaoigwHolSo2GD6pYTRUTOoZbUmvXJo\ndhoDMUJDwqyflAGYA6vNyrSntfXbq38e95a3PwQtAVSwjELm/v6hb30ZlL6Rwlg2gSLQvb7vn9bS\n3JuhP6EzFmsIEoF77ptB853Sd05oitRJrcXp5/5zY9Gwb95fR8dlKiCZh799K2Hdm4e7T1s6ZTIi\nZ/aq11hc2nF/Hw0h1wEj4awX50892vS/v/q3YIl5JrK0WgfeL32wu+c/WJyZKsLcaq/4+uziuXfF\ni0QkyuhsAIWwQIb5G+4PkPZk6gQSgNCE7mXvrUAgjQcx0Fh7Pzn0DgJGBLAKvHINPgj56GpRNigK\nnBLAXHDHpaI6YjkXw3/lZCrFAajoBvhcdFMJUk11FEzHE3/3a22yYFPsKK0m7vrHS3Qjx2EE48rL\ng7dEYvsYqSKCXnjVX2lrg9kPJpOiAVnIAP8B0Kx+GvoyGWQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x2C151BF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABv0lEQVR4nMWQv2tTURTHP+/de999\nN0lfTAtFxYjgIGlBRbCI4qB06OgmiA4FHUQHQfwXdLEiipObUKGIqx1FcFFBqYqDgm1VCgmmocH8\nfnnHIW1Noruf6XC+fM/5ngObWMAzgIG07vU8tkl5dVEuNnW6eF5CH1m7XeYIrGIQXytf6R38g2Br\ngcmTSrtgaGeQxCi7c2rv/cZfVuPBkTufEpG5sBe+zwcnHq5J/O1Na+Vwpn8iwO7bHWnLq/H9kpwG\nNyhelPVWvHQomihVT2p8AL2lRUfJrny59J1CZJsmVkm/WL374/nrNq6Rs6Vcw7aG4zIGoX4g8VTY\n6+g/YseUr+9Zlenau48JbuDWXaSPP+s2RcofJjyyQ1Nnvsrqz4XKhhRvEPXSgrIoRzhTl7mDabck\nv2T9igEfDQ4slluSLBry7+vFe4/ajflzk8cIRsEntOFNqcy68cLb1udp351fWJPuch5Aa5iVytno\n1NN67fIkQHDhyeI+7CgGZ6hL6UVZ2nItT9aGCixjWQAFIy+XRTbmrx7I4AA/CDRkwCOqEoycKZQf\nN4tEVROndC0GMB0DhKnNV2lFCgOApzQ24D/wGy4wkaNs6rMKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x2C15ACD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABKUlEQVR4nLWSPU4DMRSEPz/b2V1C\nQkGQKKjhAKRC1EjcgItwG+5AgZCoqBEngA5FQIEoEkii7I/9KIhQHJaSKf1pRjNPhqVycsDRKgM9\n347AWgTjzOqb/DgDRtTIb9sy90Kft1tZn+7+k05PWmM/mI0epr7TCilYjFynaocV9jAfGyjI16GE\nzYGld3ogJQZsAjPKz8uJPRu6aMJqIoAUMJg3eu6xYEzijALvRaVN7RSspoVmmSlCVs9xER/TQhm1\nVlNxlggS09jSxDwULDZocEqW3q8Aq1W8Auw6w0M+0np8lJsOXtJCAcMt1dbxQhtCegQb0cV1LeUQ\nQOsk1QLsPWrztuu82HRKAHi9Q3b6of5ektxQiDeTkkqhSXcaEMv9iw/QhT8/4n/rCxhxYhoGTVan\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x33961530>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAByklEQVR4nGWSPWsUURiFn7kf87Hr\nbGYjrLgSURBSSFARq4hCIoggiCIoiATEXrDOP7BKqb9AFNQ02lgEUtmkUVRsBF0SEIMrOzv7dWdf\ni90VZnKaC+fcc3jPvS/MYMGgIqqYEE0JXoitE014ryBp0OCjICQJVNEYKxOCwQs8DsT6BmgQgT1O\nvag1YenMenvgRDovQ8KiuvjkR6slImlH5PnRUuxbSSXPJBPpOndBA5j/4ua16vc3w55ZOR3oXpgX\njHW9cKoJVsX7kr4mKTj/zP/EB797VXWrn5N26RHUYVTCva19GcXloh4a1nbkl7jtIzNqiujk9VUT\nLLuxnz3cbpWanPs0FpFxNpAXKKVqBXFpty+ZdMU5uRJPOaNzPKGSfbx8YysfKLm73mt00NOe0eTw\nqaM0USOVFayaA1B4UK1QGxrwEnq3UkJG478AWCpA/VICFdCPd0dy084+xaBic3szPxExrxv393LZ\nOxvCHIBxdtR3jy7yoNYVvbpYc6/efxnAJBYLdmc4lGFbeuK+PWsCs5oxId7yhqR9cV8/PD0PVC0E\nTMZVh2DhTv/3u7Vj5b2yoIAaJgCi0u54oHwwvuEgtA9BML1oCs5/EvOZqJ08jwkAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x2C15ACD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABJUlEQVR4nHXQvUoDQRSG4Xd+dmd/\nk2JDYhoRxEawt7bzIrwjL8bawluwDoiK2ArBZJPN7syO1SIbx68a5uF8HA4MWcKMhGAMOjv+k8PD\n0juyPDxJAeYfQlEBugjrDDW9fQxbQkXiNifBhZryK5n3ahFEtjTtLu2DmHhBPPF5ECUpLeIyiHsU\n60+ugujVjmbVmyBK7dDr7yaI6gAYeR5ER4lNJ9dB1FEt4qeNiTA5IEbYdr1vX0uWHGrSxI8w0rGM\n3ijnkMt9Y0bY2bbvVs/2puJgwPI39/tTYSSRHi8Ui1jFYqM/vEjprBrXeutaf9HeFU2tDG5UmFGg\neLdnKIUU49rddItbPLzUqXMD/l4jxxgyMgVCHaFJQKMAEJKjSIwmz4bTAfwALTxVf3rEL/EAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x339612B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABi0lEQVR4nL2RzUuUYRTFf/f5fOfD\nGWNETUkKwjYJ7loIrtoEDblvq7hp0bK0bX+A+C9ErRJxk7goDHdSYDAbiyIIJATJz1EZ39vCd6Zi\n3nV39XDPc8499x7oqhIe190GwAA9SPv5TyUpmHDZl25qWTh0rVxVMQaqG+u5slpOpf/xnUYu00bi\nXPO0ngsGfHyrq+6P9czZg+V+Epje+XSv89k5JAQqbOttRy/v9eElxQiQohfn/uDq+f5oy5yNT6SB\nFACfbVrwPDv+LCS81jVPrTNMShHsyNeLySuBlyc/kHJnpIgDGfuoC8D8UWvG49ugcUBtek9189G1\nwXV94algs72IMFDXVvNDqis/9ZulTFs2WqKt66/v929taPPsSLeHHcUOhjfvvsxWhLE3p6qHS9W/\nDpaYwNxN4Prkjt4dHakRsjwxCFAE+ljURq+LwZZcBgaAPqoFeKKNG2Q+k0y2GMCADO3q09iDw7jg\n26EYYMg4tvZfFSAJAJknCy4YLDxPpyiCWO8o5Wb9P+o3c4pj5PdFLUsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x2C15ACD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.misc\n",
    "for i in range(0,10):\n",
    "    scipy.misc.toimage( np.reshape(Z.a[i,1:785], (28,28)) ).save('out'+ str(i) +'.jpg')\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "for i in range(0,10):\n",
    "    im = Image.open('./out'+str(i)+'.jpg')\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full precision training using the CPU\n",
    "\n",
    "Let's perform training on full-precision data using the CPU. With the mnist data set we want to classify between 10 classes (handwritten digits from 0 to 9). For this reason, we perform a 1-vs-all training: For each digit we train a seperate L2-SVM model, that should classify the digit compared to all others.\n",
    "\n",
    "During the training, we calculate the loss after each epoch (a complete scan over the data set). The convergence of the model is observed by the decreasing loss. After having performed the given number of epochs, we save the trained model in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b binarized for 0, time: 0.4593064785003662\n",
      "Training time: 21.559975147247314\n",
      "Initial loss: 7249.75\n",
      "Epoch 0 loss: 407.051758304\n",
      "Epoch 1 loss: 322.004559011\n",
      "Epoch 2 loss: 282.7098222\n",
      "Epoch 3 loss: 258.917717091\n",
      "Epoch 4 loss: 242.779521611\n",
      "Epoch 5 loss: 230.881771687\n",
      "Epoch 6 loss: 221.562047428\n",
      "Epoch 7 loss: 214.064932173\n",
      "Epoch 8 loss: 207.702874743\n",
      "Epoch 9 loss: 202.129375769\n",
      "b binarized for 1, time: 0.4598066806793213\n",
      "Training time: 21.79761576652527\n",
      "Initial loss: 7218.25\n",
      "Epoch 0 loss: 428.357593619\n",
      "Epoch 1 loss: 345.685836409\n",
      "Epoch 2 loss: 312.849864526\n",
      "Epoch 3 loss: 294.124601755\n",
      "Epoch 4 loss: 281.348371555\n",
      "Epoch 5 loss: 271.739573022\n",
      "Epoch 6 loss: 264.078599304\n",
      "Epoch 7 loss: 257.732066977\n",
      "Epoch 8 loss: 252.334974374\n",
      "Epoch 9 loss: 247.658311389\n",
      "b binarized for 2, time: 0.4562089443206787\n",
      "Training time: 25.91661763191223\n",
      "Initial loss: 7252.25\n",
      "Epoch 0 loss: 764.145994427\n",
      "Epoch 1 loss: 645.997702502\n",
      "Epoch 2 loss: 598.404771318\n",
      "Epoch 3 loss: 570.738883041\n",
      "Epoch 4 loss: 552.038573278\n",
      "Epoch 5 loss: 538.167857365\n",
      "Epoch 6 loss: 527.220972646\n",
      "Epoch 7 loss: 518.24695661\n",
      "Epoch 8 loss: 510.701914228\n",
      "Epoch 9 loss: 504.215946638\n",
      "b binarized for 3, time: 0.45819807052612305\n",
      "Training time: 26.99597430229187\n",
      "Initial loss: 7242.0\n",
      "Epoch 0 loss: 888.720953365\n",
      "Epoch 1 loss: 772.957794521\n",
      "Epoch 2 loss: 722.336126046\n",
      "Epoch 3 loss: 691.614436335\n",
      "Epoch 4 loss: 669.71145023\n",
      "Epoch 5 loss: 652.738762894\n",
      "Epoch 6 loss: 638.934339463\n",
      "Epoch 7 loss: 627.374933519\n",
      "Epoch 8 loss: 617.436396304\n",
      "Epoch 9 loss: 608.785961599\n",
      "b binarized for 4, time: 0.45839595794677734\n",
      "Training time: 23.953608512878418\n",
      "Initial loss: 7255.0\n",
      "Epoch 0 loss: 706.73854473\n",
      "Epoch 1 loss: 566.285529707\n",
      "Epoch 2 loss: 507.099576714\n",
      "Epoch 3 loss: 473.71913364\n",
      "Epoch 4 loss: 451.68298558\n",
      "Epoch 5 loss: 435.817990398\n",
      "Epoch 6 loss: 423.685313008\n",
      "Epoch 7 loss: 413.928170798\n",
      "Epoch 8 loss: 405.8679999\n",
      "Epoch 9 loss: 399.028258829\n",
      "b binarized for 5, time: 0.45618772506713867\n",
      "Training time: 27.14194893836975\n",
      "Initial loss: 7284.25\n",
      "Epoch 0 loss: 957.344726487\n",
      "Epoch 1 loss: 805.935107126\n",
      "Epoch 2 loss: 734.558612018\n",
      "Epoch 3 loss: 692.329596622\n",
      "Epoch 4 loss: 663.923304328\n",
      "Epoch 5 loss: 642.889547326\n",
      "Epoch 6 loss: 626.46542143\n",
      "Epoch 7 loss: 613.016333761\n",
      "Epoch 8 loss: 601.781164494\n",
      "Epoch 9 loss: 592.127018521\n",
      "b binarized for 6, time: 0.4551234245300293\n",
      "Training time: 22.380094051361084\n",
      "Initial loss: 7246.5\n",
      "Epoch 0 loss: 476.989664334\n",
      "Epoch 1 loss: 385.299227755\n",
      "Epoch 2 loss: 349.164287438\n",
      "Epoch 3 loss: 328.339916101\n",
      "Epoch 4 loss: 314.172258319\n",
      "Epoch 5 loss: 303.659592488\n",
      "Epoch 6 loss: 295.381276001\n",
      "Epoch 7 loss: 288.584141067\n",
      "Epoch 8 loss: 282.831264524\n",
      "Epoch 9 loss: 277.851922366\n",
      "b binarized for 7, time: 0.4618384838104248\n",
      "Training time: 23.453986883163452\n",
      "Initial loss: 7232.5\n",
      "Epoch 0 loss: 579.385545979\n",
      "Epoch 1 loss: 496.315782169\n",
      "Epoch 2 loss: 460.284459137\n",
      "Epoch 3 loss: 437.886257984\n",
      "Epoch 4 loss: 421.804446633\n",
      "Epoch 5 loss: 409.466573363\n",
      "Epoch 6 loss: 399.502027615\n",
      "Epoch 7 loss: 391.126071695\n",
      "Epoch 8 loss: 384.050642664\n",
      "Epoch 9 loss: 377.968936541\n",
      "b binarized for 8, time: 0.45465993881225586\n",
      "Training time: 30.911251306533813\n",
      "Initial loss: 7264.0\n",
      "Epoch 0 loss: 1299.33273283\n",
      "Epoch 1 loss: 1137.81398058\n",
      "Epoch 2 loss: 1053.647013\n",
      "Epoch 3 loss: 998.866907069\n",
      "Epoch 4 loss: 958.52315908\n",
      "Epoch 5 loss: 927.058257693\n",
      "Epoch 6 loss: 901.66565184\n",
      "Epoch 7 loss: 880.488455233\n",
      "Epoch 8 loss: 862.739290979\n",
      "Epoch 9 loss: 847.579465293\n",
      "b binarized for 9, time: 0.45929598808288574\n",
      "Training time: 27.782022953033447\n",
      "Initial loss: 7255.5\n",
      "Epoch 0 loss: 1158.9716746\n",
      "Epoch 1 loss: 999.397604999\n",
      "Epoch 2 loss: 922.877932622\n",
      "Epoch 3 loss: 875.702009007\n",
      "Epoch 4 loss: 842.65492643\n",
      "Epoch 5 loss: 817.642769415\n",
      "Epoch 6 loss: 797.699949083\n",
      "Epoch 7 loss: 781.254096791\n",
      "Epoch 8 loss: 767.344157199\n",
      "Epoch 9 loss: 755.328095684\n"
     ]
    }
   ],
   "source": [
    "xs_CPU = np.zeros((Z.num_features, 10))\n",
    "for c in range(0, 10):\n",
    "\tstart = time.time()\n",
    "\tZ.b_binarize(c) # Binarize the labels of the data set\n",
    "\tprint('b binarized for ' + str(c) + \", time: \" + str(time.time()-start) )\n",
    "\tstart = time.time()\n",
    "    \n",
    "    # Train model on the CPU\n",
    "\tx_history = Z.L2SVM_SGD(num_epochs, step_size, cost_pos, cost_neg, regularize=0, use_binarized=1)\n",
    "    \n",
    "\tprint('Training time: ' + str(time.time()-start) )\n",
    "    # Print losses after each epoch\n",
    "\tinitial_loss = Z.calculate_L2SVM_loss(np.zeros(Z.num_features), cost_pos, cost_neg, 0, 1)\n",
    "\tprint('Initial loss: ' + str(initial_loss))\n",
    "\tfor e in range(0, num_epochs):\n",
    "\t\tloss = Z.calculate_L2SVM_loss(x_history[:,e], cost_pos, cost_neg, 0, 1)\n",
    "\t\tprint('Epoch ' + str(e) + ' loss: ' + str(loss) )\n",
    "\n",
    "\txs_CPU[:,c] = x_history[:,num_epochs-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full-precision training using the FPGA\n",
    "\n",
    "Now, let's use the ZipML-PYNQ overlay to perform the same training process on the FPGA. We just replace the CPU-training function with the FPGA one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b binarized for 0, time: 0.452847957611084\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x0\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5656685829162598\n",
      "Initial loss: 7249.75\n",
      "Epoch 0 loss: 407.21395011\n",
      "Epoch 1 loss: 322.181103171\n",
      "Epoch 2 loss: 282.834546258\n",
      "Epoch 3 loss: 259.025670617\n",
      "Epoch 4 loss: 242.873300504\n",
      "Epoch 5 loss: 230.962467829\n",
      "Epoch 6 loss: 221.631189245\n",
      "Epoch 7 loss: 214.125792615\n",
      "Epoch 8 loss: 207.756877901\n",
      "Epoch 9 loss: 202.17128729\n",
      "b binarized for 1, time: 0.4604675769805908\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x3f800000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5233240127563477\n",
      "Initial loss: 7218.25\n",
      "Epoch 0 loss: 428.428984678\n",
      "Epoch 1 loss: 345.724257876\n",
      "Epoch 2 loss: 312.884168104\n",
      "Epoch 3 loss: 294.155814688\n",
      "Epoch 4 loss: 281.37468518\n",
      "Epoch 5 loss: 271.76528027\n",
      "Epoch 6 loss: 264.103019885\n",
      "Epoch 7 loss: 257.755295029\n",
      "Epoch 8 loss: 252.356938518\n",
      "Epoch 9 loss: 247.678579849\n",
      "b binarized for 2, time: 0.46558070182800293\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40000000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5240135192871094\n",
      "Initial loss: 7252.25\n",
      "Epoch 0 loss: 764.247057177\n",
      "Epoch 1 loss: 646.084133059\n",
      "Epoch 2 loss: 598.468362545\n",
      "Epoch 3 loss: 570.786949839\n",
      "Epoch 4 loss: 552.076957144\n",
      "Epoch 5 loss: 538.198583631\n",
      "Epoch 6 loss: 527.246453702\n",
      "Epoch 7 loss: 518.268794528\n",
      "Epoch 8 loss: 510.719656096\n",
      "Epoch 9 loss: 504.230104334\n",
      "b binarized for 3, time: 0.46628761291503906\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40400000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5233910083770752\n",
      "Initial loss: 7242.0\n",
      "Epoch 0 loss: 888.998291322\n",
      "Epoch 1 loss: 773.133156215\n",
      "Epoch 2 loss: 722.464735996\n",
      "Epoch 3 loss: 691.749143533\n",
      "Epoch 4 loss: 669.80997011\n",
      "Epoch 5 loss: 652.840097921\n",
      "Epoch 6 loss: 639.036707993\n",
      "Epoch 7 loss: 627.473089143\n",
      "Epoch 8 loss: 617.535322699\n",
      "Epoch 9 loss: 608.88477491\n",
      "b binarized for 4, time: 0.46358323097229004\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40800000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5242974758148193\n",
      "Initial loss: 7255.0\n",
      "Epoch 0 loss: 707.013312877\n",
      "Epoch 1 loss: 566.481014363\n",
      "Epoch 2 loss: 507.198286918\n",
      "Epoch 3 loss: 473.801126289\n",
      "Epoch 4 loss: 451.745447837\n",
      "Epoch 5 loss: 435.871522424\n",
      "Epoch 6 loss: 423.724038134\n",
      "Epoch 7 loss: 413.959596698\n",
      "Epoch 8 loss: 405.895078227\n",
      "Epoch 9 loss: 399.05231526\n",
      "b binarized for 5, time: 0.46531081199645996\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40a00000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5230717658996582\n",
      "Initial loss: 7284.25\n",
      "Epoch 0 loss: 957.350628199\n",
      "Epoch 1 loss: 805.904706737\n",
      "Epoch 2 loss: 734.537382585\n",
      "Epoch 3 loss: 692.262926353\n",
      "Epoch 4 loss: 663.881548343\n",
      "Epoch 5 loss: 642.879415927\n",
      "Epoch 6 loss: 626.464959291\n",
      "Epoch 7 loss: 613.020575015\n",
      "Epoch 8 loss: 601.789344665\n",
      "Epoch 9 loss: 592.132353608\n",
      "b binarized for 6, time: 0.4620811939239502\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40c00000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.523777961730957\n",
      "Initial loss: 7246.5\n",
      "Epoch 0 loss: 477.050554872\n",
      "Epoch 1 loss: 385.322386719\n",
      "Epoch 2 loss: 349.177711073\n",
      "Epoch 3 loss: 328.348789307\n",
      "Epoch 4 loss: 314.181434247\n",
      "Epoch 5 loss: 303.667043477\n",
      "Epoch 6 loss: 295.387393695\n",
      "Epoch 7 loss: 288.588045602\n",
      "Epoch 8 loss: 282.834008983\n",
      "Epoch 9 loss: 277.853614856\n",
      "b binarized for 7, time: 0.4653043746948242\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40e00000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5234458446502686\n",
      "Initial loss: 7232.5\n",
      "Epoch 0 loss: 579.501088596\n",
      "Epoch 1 loss: 496.396120698\n",
      "Epoch 2 loss: 460.344226423\n",
      "Epoch 3 loss: 437.932559819\n",
      "Epoch 4 loss: 421.848915944\n",
      "Epoch 5 loss: 409.505773851\n",
      "Epoch 6 loss: 399.536207688\n",
      "Epoch 7 loss: 391.158004175\n",
      "Epoch 8 loss: 384.078685428\n",
      "Epoch 9 loss: 377.993936448\n",
      "b binarized for 8, time: 0.4631478786468506\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x41000000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5236554145812988\n",
      "Initial loss: 7264.0\n",
      "Epoch 0 loss: 1299.73327457\n",
      "Epoch 1 loss: 1138.04905185\n",
      "Epoch 2 loss: 1053.89322871\n",
      "Epoch 3 loss: 999.136401729\n",
      "Epoch 4 loss: 958.828490143\n",
      "Epoch 5 loss: 927.383364616\n",
      "Epoch 6 loss: 902.010265524\n",
      "Epoch 7 loss: 880.886630976\n",
      "Epoch 8 loss: 863.115257007\n",
      "Epoch 9 loss: 847.95027393\n",
      "b binarized for 9, time: 0.46622133255004883\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x41100000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 0, self.data_end_index: 7880000\n",
      "bytes_for_model: 3144\n",
      "Training time: 0.5232534408569336\n",
      "Initial loss: 7255.5\n",
      "Epoch 0 loss: 1159.06584144\n",
      "Epoch 1 loss: 999.494802179\n",
      "Epoch 2 loss: 922.969621656\n",
      "Epoch 3 loss: 875.786435425\n",
      "Epoch 4 loss: 842.726261446\n",
      "Epoch 5 loss: 817.70494724\n",
      "Epoch 6 loss: 797.753295411\n",
      "Epoch 7 loss: 781.30000115\n",
      "Epoch 8 loss: 767.384274376\n",
      "Epoch 9 loss: 755.366683307\n"
     ]
    }
   ],
   "source": [
    "xs_floatFSGD = np.zeros((Z.num_features, 10))\n",
    "for c in range(0, 10):\n",
    "\tstart = time.time()\n",
    "\tZ.b_binarize(c)\n",
    "\tprint('b binarized for ' + str(c) + \", time: \" + str(time.time()-start) )\n",
    "\tstart = time.time()\n",
    "\t\n",
    "    # Train model on the FPGA\n",
    "\tZ.configure_SGD_FPGA(num_epochs, step_size, cost_pos, cost_neg, 1, c)\n",
    "\tx_history = Z.SGD_FPGA(num_epochs)\n",
    "\n",
    "\tprint('Training time: ' + str(time.time()-start) )\n",
    "    # Print losses after each epoch\n",
    "\tinitial_loss = Z.calculate_L2SVM_loss(np.zeros(Z.num_features), cost_pos, cost_neg, 0, 1)\n",
    "\tprint('Initial loss: ' + str(initial_loss))\n",
    "\tfor e in range(0, num_epochs):\n",
    "\t\tloss = Z.calculate_L2SVM_loss(x_history[:,e], cost_pos, cost_neg, 0, 1)\n",
    "\t\tprint('Epoch ' + str(e) + ' loss: ' + str(loss) )\n",
    "\n",
    "\txs_floatFSGD[:,c] = x_history[:,num_epochs-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training time on the FPGA is approximately 0.6 seconds for one digit, whereas on the CPU it was 20 secons. 2 orders of magnitude speedup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Low-precision training using the FPGA\n",
    "\n",
    "With the ZipML-PYNQ overlay, we can even improve upon previous results by using low-precision data. We call the quantization function that compresses the features of the data set using deterministic quantization. The rest of the code remains exactly the same as the previous example. We can see that the total training time is improved X times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_levels: 2\n",
      "qFSGD1.bit is loaded\n",
      "num_quantized_items_in_word: 32.0\n",
      "mask: 1\n",
      "self.total_size: 280000\n",
      "b binarized for 0, time: 0.45632052421569824\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x0\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.14095163345336914\n",
      "Initial loss: 7249.75\n",
      "Epoch 0 loss: 405.620035875\n",
      "Epoch 1 loss: 318.464490402\n",
      "Epoch 2 loss: 279.054448444\n",
      "Epoch 3 loss: 255.509053215\n",
      "Epoch 4 loss: 239.283327675\n",
      "Epoch 5 loss: 227.132768047\n",
      "Epoch 6 loss: 217.552909051\n",
      "Epoch 7 loss: 209.784909852\n",
      "Epoch 8 loss: 203.352229473\n",
      "Epoch 9 loss: 197.871875076\n",
      "b binarized for 1, time: 0.4523048400878906\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x3f800000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.14049386978149414\n",
      "Initial loss: 7218.25\n",
      "Epoch 0 loss: 434.454025393\n",
      "Epoch 1 loss: 353.236780034\n",
      "Epoch 2 loss: 319.968800065\n",
      "Epoch 3 loss: 300.564382341\n",
      "Epoch 4 loss: 287.051114674\n",
      "Epoch 5 loss: 276.955432347\n",
      "Epoch 6 loss: 268.941813418\n",
      "Epoch 7 loss: 262.301104666\n",
      "Epoch 8 loss: 256.726271648\n",
      "Epoch 9 loss: 251.927054979\n",
      "b binarized for 2, time: 0.45284104347229004\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40000000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.14072370529174805\n",
      "Initial loss: 7252.25\n",
      "Epoch 0 loss: 765.68796779\n",
      "Epoch 1 loss: 648.857886278\n",
      "Epoch 2 loss: 601.268200983\n",
      "Epoch 3 loss: 573.44841342\n",
      "Epoch 4 loss: 554.548554934\n",
      "Epoch 5 loss: 540.522981837\n",
      "Epoch 6 loss: 529.492989249\n",
      "Epoch 7 loss: 520.48254344\n",
      "Epoch 8 loss: 512.906480513\n",
      "Epoch 9 loss: 506.408143406\n",
      "b binarized for 3, time: 0.45859575271606445\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40400000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.14028096199035645\n",
      "Initial loss: 7242.0\n",
      "Epoch 0 loss: 894.393872376\n",
      "Epoch 1 loss: 780.721512479\n",
      "Epoch 2 loss: 730.747813492\n",
      "Epoch 3 loss: 700.459356934\n",
      "Epoch 4 loss: 678.658316233\n",
      "Epoch 5 loss: 661.634346024\n",
      "Epoch 6 loss: 647.548381138\n",
      "Epoch 7 loss: 635.723129\n",
      "Epoch 8 loss: 625.651204582\n",
      "Epoch 9 loss: 616.890448784\n",
      "b binarized for 4, time: 0.45880675315856934\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40800000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.14089393615722656\n",
      "Initial loss: 7255.0\n",
      "Epoch 0 loss: 708.276207082\n",
      "Epoch 1 loss: 572.634279778\n",
      "Epoch 2 loss: 514.563834965\n",
      "Epoch 3 loss: 481.565282401\n",
      "Epoch 4 loss: 459.457234525\n",
      "Epoch 5 loss: 443.228944097\n",
      "Epoch 6 loss: 430.697739418\n",
      "Epoch 7 loss: 420.505869485\n",
      "Epoch 8 loss: 411.978832434\n",
      "Epoch 9 loss: 404.742828629\n",
      "b binarized for 5, time: 0.45561957359313965\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40a00000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.1405496597290039\n",
      "Initial loss: 7284.25\n",
      "Epoch 0 loss: 956.838290138\n",
      "Epoch 1 loss: 811.845018351\n",
      "Epoch 2 loss: 744.772378906\n",
      "Epoch 3 loss: 704.717104831\n",
      "Epoch 4 loss: 676.771826475\n",
      "Epoch 5 loss: 656.161664037\n",
      "Epoch 6 loss: 640.338587217\n",
      "Epoch 7 loss: 627.450667749\n",
      "Epoch 8 loss: 616.598638852\n",
      "Epoch 9 loss: 607.243616657\n",
      "b binarized for 6, time: 0.4556725025177002\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40c00000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.1420118808746338\n",
      "Initial loss: 7246.5\n",
      "Epoch 0 loss: 479.569728547\n",
      "Epoch 1 loss: 387.211759329\n",
      "Epoch 2 loss: 350.516028659\n",
      "Epoch 3 loss: 329.422276837\n",
      "Epoch 4 loss: 315.231443943\n",
      "Epoch 5 loss: 304.717823298\n",
      "Epoch 6 loss: 296.469065668\n",
      "Epoch 7 loss: 289.670397097\n",
      "Epoch 8 loss: 283.924726947\n",
      "Epoch 9 loss: 278.941034022\n",
      "b binarized for 7, time: 0.45525264739990234\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x40e00000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.14040708541870117\n",
      "Initial loss: 7232.5\n",
      "Epoch 0 loss: 586.11873177\n",
      "Epoch 1 loss: 502.332380989\n",
      "Epoch 2 loss: 465.519123859\n",
      "Epoch 3 loss: 442.908113068\n",
      "Epoch 4 loss: 426.949804007\n",
      "Epoch 5 loss: 414.663377845\n",
      "Epoch 6 loss: 404.971604011\n",
      "Epoch 7 loss: 397.002568124\n",
      "Epoch 8 loss: 390.284289882\n",
      "Epoch 9 loss: 384.487177317\n",
      "b binarized for 8, time: 0.4570455551147461\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x41000000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.14078521728515625\n",
      "Initial loss: 7264.0\n",
      "Epoch 0 loss: 1298.57683662\n",
      "Epoch 1 loss: 1134.72634915\n",
      "Epoch 2 loss: 1048.8478579\n",
      "Epoch 3 loss: 992.104757925\n",
      "Epoch 4 loss: 950.185505434\n",
      "Epoch 5 loss: 917.607909793\n",
      "Epoch 6 loss: 891.258540441\n",
      "Epoch 7 loss: 869.511318354\n",
      "Epoch 8 loss: 851.240924482\n",
      "Epoch 9 loss: 835.654886414\n",
      "b binarized for 9, time: 0.4566342830657959\n",
      "Sent\n",
      "Config Received\n",
      "0x20\n",
      "0x20000\n",
      "0x39c00000\n",
      "0x39800000\n",
      "0x41100000\n",
      "0x311\n",
      "0x2710\n",
      "0xa\n",
      "0x39800000\n",
      "self.data_start_index: 7880000, self.data_end_index: 8160000\n",
      "bytes_for_model: 7424\n",
      "Training time: 0.14073395729064941\n",
      "Initial loss: 7255.5\n",
      "Epoch 0 loss: 1159.19146257\n",
      "Epoch 1 loss: 1004.7645263\n",
      "Epoch 2 loss: 929.801589588\n",
      "Epoch 3 loss: 883.11942511\n",
      "Epoch 4 loss: 850.539472761\n",
      "Epoch 5 loss: 825.923988783\n",
      "Epoch 6 loss: 806.360786689\n",
      "Epoch 7 loss: 790.304897068\n",
      "Epoch 8 loss: 776.784267527\n",
      "Epoch 9 loss: 765.265385506\n"
     ]
    }
   ],
   "source": [
    "# Quantize the features of the data set\n",
    "Z.a_quantize(quantization_bits=1)\n",
    "\n",
    "xs_qFSGD = np.zeros((Z.num_features, 10))\n",
    "for c in range(0, 10):\n",
    "\tstart = time.time()\n",
    "\tZ.b_binarize(c)\n",
    "\tprint('b binarized for ' + str(c) + \", time: \" + str(time.time()-start) )\n",
    "\tstart = time.time()\n",
    "\t\n",
    "    # Train model on the FPGA\n",
    "\tZ.configure_SGD_FPGA(num_epochs, step_size, cost_pos, cost_neg, 1, c)\n",
    "\tx_history = Z.SGD_FPGA(num_epochs)\n",
    "\n",
    "\tprint('Training time: ' + str(time.time()-start) )\n",
    "    # Print losses after each epoch\n",
    "\tinitial_loss = Z.calculate_L2SVM_loss(np.zeros(Z.num_features), cost_pos, cost_neg, 0, 1)\n",
    "\tprint('Initial loss: ' + str(initial_loss))\n",
    "\tfor e in range(0, num_epochs):\n",
    "\t\tloss = Z.calculate_L2SVM_loss(x_history[:,e], cost_pos, cost_neg, 0, 1)\n",
    "\t\tprint('Epoch ' + str(e) + ' loss: ' + str(loss) )\n",
    "\n",
    "\txs_qFSGD[:,c] = x_history[:,num_epochs-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With quantized data (here we used 1-bit) we can accelerate training even more. Here, the training for 1 digit takes approximately 0.14 seconds. The reason why we can't observe the theoretical limit of 32x speedup (1-bit vs single-precision floating-point) here is the DMA initiation overheads and the fact that we are transferring the whole model (remember, the model is still in full precision) back after each epoch.\n",
    "\n",
    "At this point, go ahead and test the trained models that you have trained with low precision data. You are going to observe that you still reach 90% classification accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained models\n",
    "\n",
    "Now that we have trained our models for each digit, we can call the classification function and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_values_one_row: 788\n",
      "CPU training -> Matches 9002 out of 10000 samples.\n",
      "FPGA training -> Matches 9003 out of 10000 samples.\n",
      "1-bit FPGA training -> Matches 9000 out of 10000 samples.\n"
     ]
    }
   ],
   "source": [
    "Z.load_libsvm_data('./mnist/mnist.t', 10000, 784)\n",
    "Z.a_normalize(to_min1_1=0, row_or_column='r');\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "matches_CPU = Z.multi_classification(xs_CPU, classes)\n",
    "matches_floatFSGD = Z.multi_classification(xs_floatFSGD, classes)\n",
    "matches_qFSGD = Z.multi_classification(xs_qFSGD, classes)\n",
    "\n",
    "print('CPU training -> Matches ' + str(matches_CPU) + ' out of ' + str(Z.num_samples) + ' samples.')\n",
    "print('FPGA training -> Matches ' + str(matches_floatFSGD) + ' out of ' + str(Z.num_samples) + ' samples.')\n",
    "print('1-bit FPGA training -> Matches ' + str(matches_qFSGD) + ' out of ' + str(Z.num_samples) + ' samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
